{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "# URL del repository pubblico\n",
    "REPO_URL = 'https://github.com/ev91/MachineInnovators_Inc_ProAI.git'\n",
    "repo_dir = pathlib.Path('/content/MachineInnovators_Inc_ProAI')\n",
    "\n",
    "if not repo_dir.exists():\n",
    "    print(f\"[*] Clonazione di {REPO_URL}...\")\n",
    "    !git clone $REPO_URL\n",
    "    print(f\"[âœ“] Repository clonato in {repo_dir}\")\n",
    "else:\n",
    "    print(f\"[*] Repository giÃ  presente in {repo_dir}\")\n",
    "\n",
    "os.chdir(repo_dir)\n",
    "print(f\"[âœ“] Working directory: {repo_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403b625",
   "metadata": {},
   "source": [
    "## 2) Installa le dipendenze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7659ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "project_root = pathlib.Path.cwd()\n",
    "\n",
    "# Verifica che il repo sia nella working directory\n",
    "if not (project_root / 'src').exists():\n",
    "    alt = pathlib.Path('/content/MachineInnovators_Inc_ProAI')\n",
    "    if alt.exists():\n",
    "        project_root = alt\n",
    "        os.chdir(project_root)\n",
    "    else:\n",
    "        raise RuntimeError('Repository non trovato: esegui la cella precedente per clonare')\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"[*] Installazione dipendenze da {project_root / 'requirements.txt'}...\")\n",
    "!pip install -q -r requirements.txt\n",
    "print(f\"[âœ“] Dipendenze installate\")\n",
    "\n",
    "# Configurazione MLflow\n",
    "os.environ['MLFLOW_TRACKING_URI'] = f'file://{project_root / \"mlruns\"}'\n",
    "os.environ['REGISTERED_MODEL_NAME'] = 'Sentiment'\n",
    "\n",
    "print(f\"[âœ“] MLflow tracking URI: {os.environ['MLFLOW_TRACKING_URI']}\")\n",
    "print(f\"[âœ“] Model name: {os.environ['REGISTERED_MODEL_NAME']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db9591",
   "metadata": {},
   "source": [
    "## 3) Demo: Inferenza con il modello HuggingFace\n",
    "\n",
    "Usiamo il modello pre-addestrato `cardiffnlp/twitter-roberta-base-sentiment-latest` per fare una predizione rapida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffac944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.serving.load_model import predict_fn\n",
    "\n",
    "# Test su diversi testi\n",
    "test_texts = [\n",
    "    \"I absolutely love this product!\",\n",
    "    \"This is just okay, nothing special.\",\n",
    "    \"Terrible experience, would not recommend.\",\n",
    "]\n",
    "\n",
    "print(\"[*] Predizioni usando il modello HuggingFace:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for text in test_texts:\n",
    "    label, score = predict_fn(text)\n",
    "    print(f\"Text: {text[:50]:50s} | Label: {label:8s} | Score: {score:.4f}\")\n",
    "\n",
    "print(\"[âœ“] Inferenza completata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb1f87",
   "metadata": {},
   "source": [
    "## 4) Addestra una nuova versione del modello\n",
    "\n",
    "Questo step registra una nuova versione del modello nel MLflow Model Registry (file-based)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[*] Avvio training...\")\n",
    "!python -m src.models.train_roberta --experiment sentiment_colab\n",
    "print(\"[âœ“] Training completato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a5043f",
   "metadata": {},
   "source": [
    "## 5) Recupera l'ultima versione registrata dal Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7cb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "model_name = os.environ.get('REGISTERED_MODEL_NAME', 'Sentiment')\n",
    "tracking_uri = os.environ['MLFLOW_TRACKING_URI']\n",
    "\n",
    "client = MlflowClient(tracking_uri=tracking_uri)\n",
    "\n",
    "# Recupera tutte le versioni\n",
    "all_versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "\n",
    "if not all_versions:\n",
    "    raise RuntimeError(f\"Nessuna versione trovata per il modello '{model_name}'\")\n",
    "\n",
    "# Prendi la versione piÃ¹ recente\n",
    "latest = sorted(all_versions, key=lambda v: int(v.version))[-1]\n",
    "new_model_uri = f\"models:/{model_name}/{latest.version}\"\n",
    "\n",
    "print(f\"[âœ“] Ultime versioni registrate:\")\n",
    "for v in sorted(all_versions[-3:], key=lambda x: int(x.version), reverse=True):\n",
    "    print(f\"  - Version {v.version}: stage={v.current_stage}\")\n",
    "\n",
    "print(f\"[âœ“] Usando versione: {new_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e784f",
   "metadata": {},
   "source": [
    "## 6) Valuta e promuovi a Production\n",
    "\n",
    "Usiamo `data/holdout.csv` come set di valutazione. Se il nuovo modello Ã¨ >= della versione in Production (o se nessuno Ã¨ in Production), viene promosso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2658e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[*] Valutazione del modello e promozione a Production...\")\n",
    "!python -m src.models.evaluate --new_model_uri $new_model_uri --eval_csv data/holdout.csv --min_improvement 0.0\n",
    "print(\"[âœ“] Valutazione e promozione completate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4a8b4",
   "metadata": {},
   "source": [
    "## 7) Carica il modello in Production dal registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d31472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.mlflow_utils import get_production_model_uri\n",
    "\n",
    "prod_uri = get_production_model_uri(model_name)\n",
    "\n",
    "if prod_uri:\n",
    "    os.environ['MODEL_URI'] = prod_uri\n",
    "    print(f\"[âœ“] Production URI trovata: {prod_uri}\")\n",
    "else:\n",
    "    os.environ['MODEL_URI'] = ''\n",
    "    print(f\"[!] Production URI non trovata, fallback a HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c67185",
   "metadata": {},
   "source": [
    "## 8) Inferenza usando il modello in Production\n",
    "\n",
    "Ora usiamo il modello registrato in Production (se disponibile) altrimenti fallback su HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a2423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload predict_fn per utilizzare il nuovo MODEL_URI\n",
    "import importlib\n",
    "import src.serving.load_model\n",
    "importlib.reload(src.serving.load_model)\n",
    "from src.serving.load_model import predict_fn\n",
    "\n",
    "print(f\"[*] Predizioni usando MODEL_URI={os.environ.get('MODEL_URI', 'None (fallback HF)')}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_texts = [\n",
    "    \"This is amazing! Best service ever!\",\n",
    "    \"Not the best, could be better.\",\n",
    "    \"Absolutely horrible, never again!\",\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    label, score = predict_fn(text)\n",
    "    print(f\"Text: {text[:50]:50s} | Label: {label:8s} | Score: {score:.4f}\")\n",
    "\n",
    "print(\"[âœ“] Inferenza completata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8b8b5",
   "metadata": {},
   "source": [
    "## 9) Sistema di Monitoraggio Continuo\n",
    "\n",
    "Questo progetto implementa un **sistema completo di monitoring** con Prometheus e Grafana per valutare continuamente:\n",
    "\n",
    "- **Performance dell'API**: volume di richieste, latenza (p50/p90), tasso d'errore\n",
    "- **Sentiment Analysis**: distribuzione dei sentiment predetti (positive/neutral/negative) nel tempo\n",
    "- **Performance del Modello**: F1-score e accuracy monitorati ad ogni retraining\n",
    "- **Data Drift Detection**: flag binario e timeline dello shift dei dati rispetto alla baseline\n",
    "\n",
    "### Flusso di Dati di Monitoraggio\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  FastAPI /predictâ”‚â”€â”€â”€â”€â”€â”€â”€ Espone metriche Prometheus â”€â”€â”€â”€â”€â”€â”\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚\n",
    "                                                             â”‚\n",
    "                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚   Prometheus      â”‚\n",
    "â”‚ Airflow DAG            â”‚â”€ Pushes model metrics â”€â”‚   (Data Store)    â”‚\n",
    "â”‚ (evaluate_and_promote) â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚\n",
    "                                                           â”‚\n",
    "                                                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                                    â”‚    Grafana    â”‚\n",
    "                                                    â”‚   Dashboard   â”‚\n",
    "                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Come Leggerlo in Tempo Reale\n",
    "\n",
    "Una volta avviato lo stack con `docker compose up`:\n",
    "\n",
    "1. **Apri Grafana**: http://localhost:3000 (login: admin/admin)\n",
    "2. **Seleziona il dashboard**: \"MLOps â€“ Sentiment Analysis Monitoring\"\n",
    "3. **Pannelli principali**:\n",
    "   - ğŸ“Š **API Request Rate**: volume di richieste al `/predict` endpoint (req/s)\n",
    "   - âš ï¸ **Error Rate**: percentuale di errori API nel tempo\n",
    "   - â±ï¸ **Latency (p50/p90)**: mediana e 90Â° percentile della latenza\n",
    "   - ğŸš¨ **Data Drift Status**: segnale rosso quando drift viene rilevato\n",
    "   - ğŸ’­ **Sentiment Distribution**: pie chart della distribuzione positive/neutral/negative (5m avg)\n",
    "   - ğŸ“ˆ **Sentiment Predictions**: andamento delle predizioni per label nel tempo\n",
    "   - ğŸ¯ **Model Performance**: F1-score e accuracy tracciati ad ogni valutazione\n",
    "   - ğŸ“Š **Latest F1 Score**: indicatore del valore attuale di F1\n",
    "   - ğŸ”„ **Predictions Distribution**: distribuzione delle predizioni nell'ultima ora\n",
    "   - ğŸ“‰ **Data Drift Timeline**: cronologia del flag di drift nel tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8399e58",
   "metadata": {},
   "source": [
    "## 10) Prossimi passi\n",
    "\n",
    "### Per eseguire lo stack completo (Airflow + Prometheus + Grafana + Monitoraggio)\n",
    "1. Clona il repository localmente\n",
    "2. Assicurati di avere Docker e Docker Compose installati (v2.0+)\n",
    "3. Esegui:\n",
    "   ```bash\n",
    "   git clone https://github.com/ev91/MachineInnovators_Inc_ProAI.git\n",
    "   cd MachineInnovators_Inc_ProAI\n",
    "   docker compose up --build\n",
    "   ```\n",
    "4. Attendi ~60-90 secondi finchÃ© tutti i servizi sono in running:\n",
    "   - **FastAPI**: http://localhost:8000 â†’ Inference endpoint\n",
    "   - **MLflow UI**: http://localhost:5000 â†’ Model Registry\n",
    "   - **Airflow UI**: http://localhost:8080 â†’ DAG orchestration\n",
    "   - **Grafana**: http://localhost:3000 (admin/admin) â†’ **Monitoring Dashboard**\n",
    "   - **Prometheus**: http://localhost:9090 â†’ Metrics database\n",
    "\n",
    "### Verifiche rapide\n",
    "\n",
    "**Health check API:**\n",
    "```bash\n",
    "curl http://localhost:8000/health\n",
    "```\n",
    "\n",
    "**Predizione di sentiment:**\n",
    "```bash\n",
    "curl -X POST http://localhost:8000/predict \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\"text\": \"I love this product!\"}'\n",
    "```\n",
    "\n",
    "**Metriche Prometheus disponibili:**\n",
    "```bash\n",
    "curl http://localhost:8000/metrics\n",
    "```\n",
    "\n",
    "### Per simulare data drift e innescare il retraining automatico\n",
    "Vedi [docs/data_drift_simulation.md](https://github.com/ev91/MachineInnovators_Inc_ProAI/blob/main/docs/data_drift_simulation.md)\n",
    "\n",
    "### Per comprendere il flusso di training, valutazione e promozione\n",
    "Vedi [docs/training_and_promotion.md](https://github.com/ev91/MachineInnovators_Inc_ProAI/blob/main/docs/training_and_promotion.md)\n",
    "\n",
    "### Documentazione completa\n",
    "Leggi il [README.md](https://github.com/ev91/MachineInnovators_Inc_ProAI) per la panoramica architetturale completa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ac6bc",
   "metadata": {},
   "source": [
    "## 11) Screenshot (evidenze)\n",
    "\n",
    "Le seguenti schermate mostrano lo stack in esecuzione. Una volta avviato con `docker compose up`, potrai accedere a questi servizi e visualizzare il monitoraggio in tempo reale del modello di sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f9f5b2",
   "metadata": {},
   "source": [
    "![Airflow DAG](../docs/screenshots/Schermata%20Airflow.png)\n",
    "\n",
    "**Airflow DAG Orchestration** (`http://localhost:8080`): Vista del DAG `retrain_sentiment` con tutti i task (ingest â†’ drift â†’ branch â†’ train â†’ evaluate_and_promote). Mostra i run completati, lo stato di ogni task e i log per verificare il flusso di retraining automatico quando viene rilevato data drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa8804",
   "metadata": {},
   "source": [
    "![Grafana Dashboard](../docs/screenshots/Schermata%20Grafana.png)\n",
    "\n",
    "**Grafana Dashboard di Monitoraggio** (`http://localhost:3000`, login: admin/admin): Dashboard \"MLOps â€“ Sentiment Analysis Monitoring\" che fornisce **visibilitÃ  continua** su:\n",
    "- **Request Rate & Error Rate**: volume di richieste API e percentuale d'errore\n",
    "- **Latency (p50/p90)**: performance dell'endpoint di inferenza\n",
    "- **Sentiment Distribution**: pie chart della proporzione positive/neutral/negative nei sentiment predetti (ultimi 5 minuti)\n",
    "- **Sentiment Predictions**: trend temporale di quanti testi sono stati classificati come positive/neutral/negative\n",
    "- **Model Performance (F1 & Accuracy)**: metriche di performance del modello nel tempo, aggiornate dopo ogni retraining\n",
    "- **Data Drift Timeline**: cronologia del flag di drift (0=no drift, 1=drift detected)\n",
    "\n",
    "Questa Ã¨ la **componente chiave** per la \"continuous monitoring\" richiesta dalla traccia d'esame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaafcea",
   "metadata": {},
   "source": [
    "![MLflow Registry](../docs/screenshots/Schermata%20MLFlow.png)\n",
    "\n",
    "**MLflow Model Registry** (`http://localhost:5000`): Model Registry con tutte le versioni del modello `Sentiment`. Mostra:\n",
    "- Versioni registrate durante il training\n",
    "- Stage corrente di ogni versione (Production/Staging/None)\n",
    "- Parametri e metriche di ogni run (accuracy, F1-score, ecc.)\n",
    "- Automaticamente aggiornato dal DAG di Airflow durante valutazione e promozione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba2bfee",
   "metadata": {},
   "source": [
    "![GitHub Actions CI](../docs/screenshots/Schermata%20Github%20Actions.png)\n",
    "\n",
    "**GitHub Actions CI/CD Pipeline**: Automaticamente eseguita su ogni push/PR con:\n",
    "- Linting (ruff, flake8)\n",
    "- Test unitari (pytest)\n",
    "- Smoke test su training e inferenza\n",
    "- Utile come prova che la pipeline Ã¨ funzionante e mantenuta"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
